In this hackathon, we train models that predict the strength and shape of interactions between the nuclear spins from simulated time-dependent magnetization curves. Two models are made depending upon the type of interaction function between the nuclear spins: Gaussian and Ruderman-Kittel-Kasuya-Yosida (RKKY) function. Our main goal here is to build a regression model to predict the three parameters: coupling strength (ùú∂), coupling length (ùû∑), and the decay time (d).

First, we built a Multi-layer Perceptron (MLP) model and trained it using the Gaussian function. We used GridsearchCV from the SciKit-Learn library to find the best hyperparameters, such as: optimizer (Adam), weight initialization (he_uniform), activation function (relu), batch size, number of epochs, and the number of neurons in the hidden layers. After finding the optimal hyperparameters, we trained the model on truncated data (which is centered roughly at the echo) and its performance was evaluated on the test dataset. We also trained the model on the entire time-axis data, and a prediction is made on the evaluation dataset. In both cases, our model‚Äôs performance didn‚Äôt vary much. The test loss remained around 0.0131.

Next, for the RKKY model prediction we just used the imaginary part of the whole magnetization M(t) curves. For a pyramid shape structure of the Neural Network (NN) layers we defined a function to calculate the number of nodes in subsequent layers. Then using the RandomizedGridSearchCV we found the best NN for RKKY model prediction is 5 dense layers with the batch normalization in between. The selected activation function was ‚Äòrelu‚Äô and for the loss function ‚Äòhuber_loss‚Äô performs the best. In addition, for the kernel initializer the grid search recommended ‚Äòlecun_uniform‚Äô. We used the early stopping callbacks and hence set up the epoch at a high value of 500. We also used the callback ‚ÄòReduceLROnPlateu‚Äô to reduce learning rate close to the optimal solution. The model generated a slightly higher loss for this function (test loss of 0.0372) as compared to the loss for the Gaussian function trained on the full time-axis data.